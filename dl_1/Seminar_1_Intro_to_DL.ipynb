{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Введение. Полносвязные слои. Функции активации (ноутбук)\n",
    "\n",
    "> Начнем осваивать библиотеку `PyTorch`. Познакомимся с нейронными сетями."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## План ноутбука\n",
    "\n",
    "1. Установка `PyTorch`\n",
    "1. Введение в `PyTorch`\n",
    "1. Полносвязные слои и функции активации в `PyTorch`\n",
    "1. Градиентный спуск своими руками"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Установка `PyTorch`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Мы будем использовать библиотеку для глубинного обучения `PyTorch`, ее можно не устанавливать, будем пользоваться сайтом [kaggle.com](kaggle.com) для обучения в облаке (или с учителем?). \n",
    "\n",
    "Чтобы установить `PyTorch` локально себе на компьютер нужно ответить на два вопроса - какая у вас операционная система и есть ли у вас дискретная видеокарта (GPU) и если есть, то какого производителя. В зависимости от ваших ответов мы получаем три варианта по операционной системе - Linux, Mac и Windows; три варианта по дискретной видеокарте - нет видеокарты (доступен только центральный процессор CPU), есть видеокарта от Nvidia или есть видеокарта от AMD (это производитель именно чипа, конечный вендор может быть другой, например, ASUS, MSI, Palit). Работа с PyTorch с видеокартой от AMD это экзотика, которая выходит за рамки нашего курса, поэтому рассмотрим только варианты *нет видеокарты*/*есть видеокарта от Nvidia*.\n",
    "\n",
    "\n",
    "Выберите на [сайте](https://pytorch.org/get-started/locally/) подходящие вам варианты операционной системы/видеокарты и скопируйте команду для установки. Разберем подробно самые популярные варианты установки:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Установка в Linux ([поддерживаемые дистрибутивы](https://pytorch.org/get-started/locally/#supported-linux-distributions))\n",
    "\n",
    "На линуксе будет работать поддержка `PyTorch` в любой конфигурации, что у вас нет видеокарты, что есть от Nvidia, что от AMD. \n",
    "\n",
    "Пререквизит для работы с видеокартой от Nvidia - нужно поставить CUDA, это инструмент от компании Nvidia, который позволяет ускорять вычисления на их же ГПУ. Чтобы поставить себе на машину все правильно воспользуйтесь этим [гайдом](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html) от Nvidia.\n",
    "\n",
    " - **pip**\n",
    "\n",
    "`pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu` для тех, у кого нет видеокарты.\n",
    "\n",
    "`pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116` для тех, у кого есть видеокарта (либо другой `--extra-index-url`, смотрите на сайте PyTorch, в зависимости от версии CUDA).\n",
    "\n",
    " - **conda**\n",
    "\n",
    "`conda install pytorch torchvision torchaudio cpuonly -c pytorch` для тех, у кого нет видеокарты.\n",
    "\n",
    "`conda install pytorch torchvision torchaudio cudatoolkit=11.6 -c pytorch -c conda-forge` для тех, у кого есть видеокарта (либо немного другая команда, в зависимости от версии CUDA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Установка в Windows\n",
    "\n",
    "На винде будет работать поддержка `PyTorch` только для видеокарт от Nvidia и без видеокарт вообще. \n",
    "\n",
    "Пререквизит для работы с видеокартой от Nvidia - нужно поставить CUDA, это инструмент от компании Nvidia, который позволяет ускорять вычисления на их же ГПУ. Чтобы поставить себе на машину все правильно воспользуйтесь этим [гайдом](https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html) от Nvidia.\n",
    "\n",
    " - **pip**\n",
    "\n",
    "`pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu` для тех, у кого нет видеокарты.\n",
    "\n",
    "`pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116` для тех, у кого есть видеокарта (либо другой `--extra-index-url`, смотрите на сайте PyTorch, в зависимости от версии CUDA).\n",
    "\n",
    " - **conda**\n",
    "\n",
    "`conda install pytorch torchvision torchaudio cpuonly -c pytorch` для тех, у кого нет видеокарты.\n",
    "\n",
    "`conda install pytorch torchvision torchaudio cudatoolkit=11.6 -c pytorch -c conda-forge` для тех, у кого есть видеокарта (либо немного другая команда, в зависимости от версии CUDA).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Установка на Mac\n",
    "\n",
    "На маках есть пока что поддержка `PyTorch` только центрального процессора, чуть позже появится поддержка ускорения на чипах M1, M2, M1 Pro и так далее.\n",
    "\n",
    " - **pip**\n",
    "\n",
    "`pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu` \n",
    "\n",
    " - **conda**\n",
    "\n",
    "`conda install pytorch torchvision torchaudio cpuonly -c pytorch`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Введение в `PyTorch`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Тензоры\n",
    "\n",
    "Тензоры — это специализированная структура данных, по сути это массивы и матрицы. Тензоры очень похожи на массивы в numpy, так что, если у вас хорошо с numpy, то разобраться в PyTorch тензорах будет очень просто. В PyTorch мы используем тензоры для кодирования входных и выходных данных модели, а также параметров модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание тензоров\n",
    "\n",
    "Тензор можно создать напрямую из каких-то данных - нам подходят все списки с числами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_data = [1, 2, 3, 4]\n",
    "some_tensor = torch.tensor(some_data)\n",
    "\n",
    "some_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4],\n",
       "        [5, 6]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_data = [[1, 2], [3, 4], [5, 6]]\n",
    "some_tensor = torch.tensor(some_data)\n",
    "\n",
    "some_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1],\n",
       "         [2]],\n",
       "\n",
       "        [[3],\n",
       "         [4]],\n",
       "\n",
       "        [[5],\n",
       "         [6]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_data = [[[1], [2]], [[3], [4]], [[5], [6]]]\n",
    "some_tensor = torch.tensor(some_data)\n",
    "\n",
    "some_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На самом деле про \"все\" списки с числами - обман. Если у вашего списка есть какой-то уровень вложенности, то должны совпадать размерности у всех вложенных списков (подробнее про размерности поговорим позже):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 2 at dim 1 (got 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m some_other_data \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m], [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m], [\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m7\u001b[39m]]\n\u001b[1;32m----> 2\u001b[0m some_other_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msome_other_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m some_other_tensor\n",
      "\u001b[1;31mValueError\u001b[0m: expected sequence of length 2 at dim 1 (got 3)"
     ]
    }
   ],
   "source": [
    "some_other_data = [[1, 2], [3, 4], [5, 6, 7]]\n",
    "some_other_tensor = torch.tensor(some_other_data)\n",
    "\n",
    "some_other_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также тензоры можно создавать из numpy массивов и наоборот:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1],\n",
       "        [2]],\n",
       "\n",
       "       [[3],\n",
       "        [4]],\n",
       "\n",
       "       [[5],\n",
       "        [6]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_numpy_array = np.array(some_data)\n",
    "\n",
    "some_numpy_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1],\n",
       "         [2]],\n",
       "\n",
       "        [[3],\n",
       "         [4]],\n",
       "\n",
       "        [[5],\n",
       "         [6]]], dtype=torch.int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_tensor_from_numpy = torch.from_numpy(some_numpy_array)\n",
    "\n",
    "some_tensor_from_numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При этом если мы создаем тензор из numpy массива с помощью `torch.from_numpy`, то они делят между собой память, где лежат их данные и, соответственно, при изменении тензора меняется numpy массив и наоборот:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=torch.float64))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.ones(10)\n",
    "y = torch.from_numpy(x)\n",
    "\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2., 2., 2., 2., 2., 2., 2., 2., 2., 2.]),\n",
       " tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2.], dtype=torch.float64))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x += 1\n",
    "\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(10)\n",
    "y = x.numpy()\n",
    "\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2.]),\n",
       " array([2., 2., 2., 2., 2., 2., 2., 2., 2., 2.], dtype=float32))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y += 1\n",
    "\n",
    "x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можем создать тензор со случайными или константными значениями:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4611, 0.1611, 0.7985],\n",
       "         [0.8779, 0.2451, 0.8234]]),\n",
       " tensor([[1., 1., 1.],\n",
       "         [1., 1., 1.]]),\n",
       " tensor([[0., 0., 0.],\n",
       "         [0., 0., 0.]]),\n",
       " tensor([[-2.2003e+12,  1.1042e-42,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape = (2, 3)\n",
    "\n",
    "random_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "empty_tensor = torch.empty(shape)\n",
    "\n",
    "random_tensor, ones_tensor, zeros_tensor, empty_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь поговорим про размерности подробнее.\n",
    "\n",
    "У тензора есть какой-то размер, какая форма. Первое с чем нужно определиться, какой **размерности** тензор - количество осей у него."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0480, 0.4912, 0.2237, 0.1224, 0.0683, 0.0298, 0.1649, 0.2274, 0.8110,\n",
       "        0.3852])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape = (10)  # одна ось (вектор)\n",
    "\n",
    "tensor = torch.rand(shape)\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8293, 0.7303, 0.6746],\n",
       "        [0.8649, 0.3587, 0.7201]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape = (2, 3)  # две оси (матрица)\n",
    "\n",
    "tensor = torch.rand(shape)\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.7301, 0.6346, 0.8161],\n",
       "         [0.5859, 0.5257, 0.9787]],\n",
       "\n",
       "        [[0.8479, 0.5375, 0.5245],\n",
       "         [0.4042, 0.7128, 0.9833]],\n",
       "\n",
       "        [[0.2602, 0.4471, 0.7631],\n",
       "         [0.2297, 0.2616, 0.3506]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape = (3, 2, 3)  # три оси (и больше - тензор)\n",
    "\n",
    "tensor = torch.rand(shape)\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тензор с размерностью 1 - это просто вектор, список чисел.\n",
    "\n",
    "Тензор с размерностью 2 - это просто матрица, то есть список списков чисел.\n",
    "\n",
    "Тензор с размерностью 3 и больше - это тензор, то есть список списков списков ... чисел."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получить доступ к размеру уже созданного тензора - метод `.shape`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1],\n",
      "         [2]],\n",
      "\n",
      "        [[3],\n",
      "         [4]],\n",
      "\n",
      "        [[5],\n",
      "         [6]]])\n",
      "torch.Size([3, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "some_data = [[[1], [2]], [[3], [4]], [[5], [6]]]\n",
    "some_tensor = torch.tensor(some_data)\n",
    "\n",
    "print(some_tensor)\n",
    "print(some_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В лекции мы говорили про изображения, давайте сделаем тензор, который будет нам имитировать изображение - сделаем его размер `(c, h, w)`, где `h` и `w` это его высота и ширина, а `c` - число каналов в цветовом пространстве (в черно-белом 1, в RGB 3):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.9122, 0.3223, 0.4666, 0.2324, 0.3122, 0.4998, 0.5459, 0.7810,\n",
       "          0.2733, 0.0281, 0.1750, 0.4092, 0.1139, 0.6847, 0.8946, 0.6382],\n",
       "         [0.8863, 0.8033, 0.5437, 0.9709, 0.0042, 0.1379, 0.7288, 0.7533,\n",
       "          0.5925, 0.9310, 0.0237, 0.2586, 0.3776, 0.8724, 0.2243, 0.6327],\n",
       "         [0.8435, 0.0556, 0.2236, 0.4691, 0.6103, 0.0689, 0.3413, 0.0066,\n",
       "          0.9621, 0.1318, 0.6359, 0.4766, 0.2403, 0.7077, 0.9671, 0.2140],\n",
       "         [0.3036, 0.7371, 0.8677, 0.2920, 0.1032, 0.2017, 0.2814, 0.2945,\n",
       "          0.6628, 0.7842, 0.7247, 0.5184, 0.1903, 0.3428, 0.0231, 0.9494],\n",
       "         [0.5257, 0.3036, 0.6584, 0.1293, 0.9509, 0.2733, 0.7146, 0.6022,\n",
       "          0.5801, 0.5890, 0.4350, 0.4201, 0.5995, 0.6261, 0.7632, 0.4032],\n",
       "         [0.4289, 0.9652, 0.2576, 0.6214, 0.5429, 0.6984, 0.8288, 0.3208,\n",
       "          0.4712, 0.2283, 0.4344, 0.4246, 0.0279, 0.5000, 0.6435, 0.6346],\n",
       "         [0.8473, 0.6080, 0.2174, 0.9534, 0.5018, 0.2599, 0.9222, 0.3532,\n",
       "          0.9801, 0.5806, 0.4164, 0.5332, 0.7666, 0.9611, 0.8883, 0.3003],\n",
       "         [0.7706, 0.5180, 0.4842, 0.8665, 0.9845, 0.9111, 0.1794, 0.7326,\n",
       "          0.7926, 0.1586, 0.8873, 0.7137, 0.1325, 0.4004, 0.7553, 0.3686],\n",
       "         [0.3338, 0.9669, 0.5291, 0.5311, 0.7705, 0.7119, 0.4769, 0.6348,\n",
       "          0.4732, 0.6979, 0.7466, 0.3747, 0.1339, 0.7085, 0.3615, 0.8979]],\n",
       "\n",
       "        [[0.5593, 0.7159, 0.4699, 0.4144, 0.9191, 0.8738, 0.7869, 0.2402,\n",
       "          0.2261, 0.8210, 0.8926, 0.1630, 0.9649, 0.4484, 0.3857, 0.4147],\n",
       "         [0.0868, 0.1717, 0.3014, 0.2822, 0.1919, 0.4792, 0.7738, 0.8596,\n",
       "          0.5153, 0.0943, 0.5576, 0.2811, 0.1214, 0.9893, 0.9017, 0.0379],\n",
       "         [0.4709, 0.7398, 0.5983, 0.9485, 0.5056, 0.9415, 0.1073, 0.8326,\n",
       "          0.0327, 0.4733, 0.5037, 0.4287, 0.6450, 0.1870, 0.5721, 0.4858],\n",
       "         [0.0946, 0.8507, 0.8002, 0.0420, 0.1934, 0.2828, 0.6703, 0.5836,\n",
       "          0.3479, 0.9002, 0.1502, 0.4752, 0.8381, 0.4001, 0.0013, 0.3185],\n",
       "         [0.7696, 0.7106, 0.6772, 0.0075, 0.1329, 0.2683, 0.9682, 0.2659,\n",
       "          0.9917, 0.7931, 0.2279, 0.2333, 0.2167, 0.2842, 0.0190, 0.8355],\n",
       "         [0.4745, 0.8209, 0.9496, 0.6284, 0.9152, 0.5258, 0.8722, 0.4971,\n",
       "          0.2839, 0.8515, 0.5454, 0.9164, 0.9341, 0.9997, 0.6718, 0.4317],\n",
       "         [0.0698, 0.8689, 0.9317, 0.4187, 0.8853, 0.5068, 0.9789, 0.1054,\n",
       "          0.4872, 0.1844, 0.4848, 0.0890, 0.1465, 0.1498, 0.8857, 0.6195],\n",
       "         [0.9459, 0.8108, 0.2347, 0.5073, 0.4616, 0.1124, 0.4333, 0.5395,\n",
       "          0.1766, 0.3478, 0.3686, 0.9058, 0.9266, 0.5205, 0.8019, 0.0449],\n",
       "         [0.1014, 0.0525, 0.6825, 0.4640, 0.9477, 0.3026, 0.5215, 0.6659,\n",
       "          0.3486, 0.7205, 0.0936, 0.4641, 0.1705, 0.1771, 0.7297, 0.1915]],\n",
       "\n",
       "        [[0.0546, 0.9813, 0.0038, 0.0842, 0.0912, 0.1330, 0.0526, 0.7534,\n",
       "          0.2598, 0.4135, 0.1003, 0.9047, 0.0600, 0.0917, 0.2770, 0.9809],\n",
       "         [0.6330, 0.7084, 0.9793, 0.9170, 0.6425, 0.9635, 0.0557, 0.3925,\n",
       "          0.3669, 0.1347, 0.6168, 0.7476, 0.7603, 0.6764, 0.6962, 0.9645],\n",
       "         [0.0627, 0.6018, 0.6934, 0.6893, 0.8140, 0.0297, 0.8216, 0.4113,\n",
       "          0.9083, 0.0860, 0.4151, 0.1995, 0.5541, 0.5653, 0.3692, 0.1165],\n",
       "         [0.4277, 0.6940, 0.1516, 0.1991, 0.7404, 0.8143, 0.0164, 0.2370,\n",
       "          0.3079, 0.4451, 0.5365, 0.5199, 0.2448, 0.7873, 0.1703, 0.7064],\n",
       "         [0.1704, 0.9694, 0.9345, 0.2936, 0.1713, 0.7533, 0.2768, 0.6060,\n",
       "          0.2266, 0.9100, 0.6748, 0.0365, 0.9343, 0.9250, 0.8429, 0.8235],\n",
       "         [0.7187, 0.9030, 0.0759, 0.3654, 0.7108, 0.4419, 0.2790, 0.6440,\n",
       "          0.1889, 0.0931, 0.5042, 0.2391, 0.6976, 0.2972, 0.9228, 0.7826],\n",
       "         [0.3284, 0.5678, 0.3422, 0.0665, 0.6937, 0.4038, 0.0449, 0.3116,\n",
       "          0.5402, 0.9409, 0.5627, 0.3471, 0.7154, 0.5239, 0.6413, 0.7104],\n",
       "         [0.3086, 0.4428, 0.1561, 0.2671, 0.2316, 0.1802, 0.9434, 0.7255,\n",
       "          0.9074, 0.0649, 0.1879, 0.0031, 0.7838, 0.6775, 0.8945, 0.2714],\n",
       "         [0.1888, 0.1902, 0.1697, 0.7848, 0.1068, 0.6183, 0.0087, 0.2328,\n",
       "          0.8005, 0.9219, 0.9638, 0.1522, 0.9515, 0.1086, 0.1770, 0.1727]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = 9\n",
    "w = 16\n",
    "c = 3\n",
    "\n",
    "shape = (c, h, w)\n",
    "\n",
    "image_tensor = torch.rand(shape)\n",
    "\n",
    "image_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 9, 16])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можем попробовать поменять размер тензора, например, [вытянуть его в вектор](https://pytorch.org/docs/stable/generated/torch.ravel.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.9122, 0.3223, 0.4666, 0.2324, 0.3122, 0.4998, 0.5459, 0.7810, 0.2733,\n",
       "         0.0281, 0.1750, 0.4092, 0.1139, 0.6847, 0.8946, 0.6382, 0.8863, 0.8033,\n",
       "         0.5437, 0.9709, 0.0042, 0.1379, 0.7288, 0.7533, 0.5925, 0.9310, 0.0237,\n",
       "         0.2586, 0.3776, 0.8724, 0.2243, 0.6327, 0.8435, 0.0556, 0.2236, 0.4691,\n",
       "         0.6103, 0.0689, 0.3413, 0.0066, 0.9621, 0.1318, 0.6359, 0.4766, 0.2403,\n",
       "         0.7077, 0.9671, 0.2140, 0.3036, 0.7371, 0.8677, 0.2920, 0.1032, 0.2017,\n",
       "         0.2814, 0.2945, 0.6628, 0.7842, 0.7247, 0.5184, 0.1903, 0.3428, 0.0231,\n",
       "         0.9494, 0.5257, 0.3036, 0.6584, 0.1293, 0.9509, 0.2733, 0.7146, 0.6022,\n",
       "         0.5801, 0.5890, 0.4350, 0.4201, 0.5995, 0.6261, 0.7632, 0.4032, 0.4289,\n",
       "         0.9652, 0.2576, 0.6214, 0.5429, 0.6984, 0.8288, 0.3208, 0.4712, 0.2283,\n",
       "         0.4344, 0.4246, 0.0279, 0.5000, 0.6435, 0.6346, 0.8473, 0.6080, 0.2174,\n",
       "         0.9534, 0.5018, 0.2599, 0.9222, 0.3532, 0.9801, 0.5806, 0.4164, 0.5332,\n",
       "         0.7666, 0.9611, 0.8883, 0.3003, 0.7706, 0.5180, 0.4842, 0.8665, 0.9845,\n",
       "         0.9111, 0.1794, 0.7326, 0.7926, 0.1586, 0.8873, 0.7137, 0.1325, 0.4004,\n",
       "         0.7553, 0.3686, 0.3338, 0.9669, 0.5291, 0.5311, 0.7705, 0.7119, 0.4769,\n",
       "         0.6348, 0.4732, 0.6979, 0.7466, 0.3747, 0.1339, 0.7085, 0.3615, 0.8979,\n",
       "         0.5593, 0.7159, 0.4699, 0.4144, 0.9191, 0.8738, 0.7869, 0.2402, 0.2261,\n",
       "         0.8210, 0.8926, 0.1630, 0.9649, 0.4484, 0.3857, 0.4147, 0.0868, 0.1717,\n",
       "         0.3014, 0.2822, 0.1919, 0.4792, 0.7738, 0.8596, 0.5153, 0.0943, 0.5576,\n",
       "         0.2811, 0.1214, 0.9893, 0.9017, 0.0379, 0.4709, 0.7398, 0.5983, 0.9485,\n",
       "         0.5056, 0.9415, 0.1073, 0.8326, 0.0327, 0.4733, 0.5037, 0.4287, 0.6450,\n",
       "         0.1870, 0.5721, 0.4858, 0.0946, 0.8507, 0.8002, 0.0420, 0.1934, 0.2828,\n",
       "         0.6703, 0.5836, 0.3479, 0.9002, 0.1502, 0.4752, 0.8381, 0.4001, 0.0013,\n",
       "         0.3185, 0.7696, 0.7106, 0.6772, 0.0075, 0.1329, 0.2683, 0.9682, 0.2659,\n",
       "         0.9917, 0.7931, 0.2279, 0.2333, 0.2167, 0.2842, 0.0190, 0.8355, 0.4745,\n",
       "         0.8209, 0.9496, 0.6284, 0.9152, 0.5258, 0.8722, 0.4971, 0.2839, 0.8515,\n",
       "         0.5454, 0.9164, 0.9341, 0.9997, 0.6718, 0.4317, 0.0698, 0.8689, 0.9317,\n",
       "         0.4187, 0.8853, 0.5068, 0.9789, 0.1054, 0.4872, 0.1844, 0.4848, 0.0890,\n",
       "         0.1465, 0.1498, 0.8857, 0.6195, 0.9459, 0.8108, 0.2347, 0.5073, 0.4616,\n",
       "         0.1124, 0.4333, 0.5395, 0.1766, 0.3478, 0.3686, 0.9058, 0.9266, 0.5205,\n",
       "         0.8019, 0.0449, 0.1014, 0.0525, 0.6825, 0.4640, 0.9477, 0.3026, 0.5215,\n",
       "         0.6659, 0.3486, 0.7205, 0.0936, 0.4641, 0.1705, 0.1771, 0.7297, 0.1915,\n",
       "         0.0546, 0.9813, 0.0038, 0.0842, 0.0912, 0.1330, 0.0526, 0.7534, 0.2598,\n",
       "         0.4135, 0.1003, 0.9047, 0.0600, 0.0917, 0.2770, 0.9809, 0.6330, 0.7084,\n",
       "         0.9793, 0.9170, 0.6425, 0.9635, 0.0557, 0.3925, 0.3669, 0.1347, 0.6168,\n",
       "         0.7476, 0.7603, 0.6764, 0.6962, 0.9645, 0.0627, 0.6018, 0.6934, 0.6893,\n",
       "         0.8140, 0.0297, 0.8216, 0.4113, 0.9083, 0.0860, 0.4151, 0.1995, 0.5541,\n",
       "         0.5653, 0.3692, 0.1165, 0.4277, 0.6940, 0.1516, 0.1991, 0.7404, 0.8143,\n",
       "         0.0164, 0.2370, 0.3079, 0.4451, 0.5365, 0.5199, 0.2448, 0.7873, 0.1703,\n",
       "         0.7064, 0.1704, 0.9694, 0.9345, 0.2936, 0.1713, 0.7533, 0.2768, 0.6060,\n",
       "         0.2266, 0.9100, 0.6748, 0.0365, 0.9343, 0.9250, 0.8429, 0.8235, 0.7187,\n",
       "         0.9030, 0.0759, 0.3654, 0.7108, 0.4419, 0.2790, 0.6440, 0.1889, 0.0931,\n",
       "         0.5042, 0.2391, 0.6976, 0.2972, 0.9228, 0.7826, 0.3284, 0.5678, 0.3422,\n",
       "         0.0665, 0.6937, 0.4038, 0.0449, 0.3116, 0.5402, 0.9409, 0.5627, 0.3471,\n",
       "         0.7154, 0.5239, 0.6413, 0.7104, 0.3086, 0.4428, 0.1561, 0.2671, 0.2316,\n",
       "         0.1802, 0.9434, 0.7255, 0.9074, 0.0649, 0.1879, 0.0031, 0.7838, 0.6775,\n",
       "         0.8945, 0.2714, 0.1888, 0.1902, 0.1697, 0.7848, 0.1068, 0.6183, 0.0087,\n",
       "         0.2328, 0.8005, 0.9219, 0.9638, 0.1522, 0.9515, 0.1086, 0.1770, 0.1727]),\n",
       " torch.Size([432]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_tensor.ravel(), image_tensor.ravel().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([432])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_tensor.ravel().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "432"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h * w * c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем количество элементов в тензоре с помощью [специальной функции](https://pytorch.org/docs/stable/generated/torch.numel.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "432"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_tensor.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1662, 0.7618, 0.7411],\n",
       "         [0.9287, 0.7698, 0.9628]],\n",
       "\n",
       "        [[0.4503, 0.8027, 0.6484],\n",
       "         [0.7300, 0.2727, 0.6985]],\n",
       "\n",
       "        [[0.3190, 0.7135, 0.4526],\n",
       "         [0.7333, 0.2365, 0.9106]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = 2\n",
    "w = 3\n",
    "c = 3\n",
    "\n",
    "shape = (c, h, w)\n",
    "\n",
    "image_tensor = torch.rand(shape)\n",
    "\n",
    "image_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем поменять размер с помощью функции [reshape](https://pytorch.org/docs/stable/generated/torch.reshape.html#torch.reshape):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1662, 0.7618, 0.7411, 0.9287, 0.7698, 0.9628],\n",
       "        [0.4503, 0.8027, 0.6484, 0.7300, 0.2727, 0.6985],\n",
       "        [0.3190, 0.7135, 0.4526, 0.7333, 0.2365, 0.9106]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_tensor.reshape(c, h * w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем собрать из нескольких тензоров один большой:\n",
    "\n",
    "[torch.cat](https://pytorch.org/docs/stable/generated/torch.cat.html#torch.cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1179, -1.2630,  1.1047],\n",
       "        [-0.9844,  0.7412, -0.1031]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1179, -1.2630,  1.1047],\n",
       "        [-0.9844,  0.7412, -0.1031],\n",
       "        [-1.1179, -1.2630,  1.1047],\n",
       "        [-0.9844,  0.7412, -0.1031],\n",
       "        [-1.1179, -1.2630,  1.1047],\n",
       "        [-0.9844,  0.7412, -0.1031]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((x, x, x), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1179, -1.2630,  1.1047, -1.1179, -1.2630,  1.1047, -1.1179, -1.2630,\n",
       "          1.1047],\n",
       "        [-0.9844,  0.7412, -0.1031, -0.9844,  0.7412, -0.1031, -0.9844,  0.7412,\n",
       "         -0.1031]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((x, x, x), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7531,  0.7948,  0.0156],\n",
      "        [-0.8526,  0.3399,  1.1618],\n",
      "        [ 0.1631, -0.7725,  0.0520]])\n",
      "tensor([[-1.1505, -0.4179, -1.4577],\n",
      "        [ 0.6881, -1.1845,  0.8080],\n",
      "        [-0.2551, -0.0740, -1.3385],\n",
      "        [ 0.1490,  0.2849, -0.1456],\n",
      "        [-0.4726,  1.5048,  1.1023]])\n",
      "tensor([[-0.5772, -0.9004, -1.4628]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7531,  0.7948,  0.0156],\n",
       "        [-0.8526,  0.3399,  1.1618],\n",
       "        [ 0.1631, -0.7725,  0.0520],\n",
       "        [-1.1505, -0.4179, -1.4577],\n",
       "        [ 0.6881, -1.1845,  0.8080],\n",
       "        [-0.2551, -0.0740, -1.3385],\n",
       "        [ 0.1490,  0.2849, -0.1456],\n",
       "        [-0.4726,  1.5048,  1.1023],\n",
       "        [-0.5772, -0.9004, -1.4628]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3, 3)\n",
    "y = torch.randn(5, 3)\n",
    "z = torch.randn(1, 3)\n",
    "\n",
    "for tensor in [x, y, z]:\n",
    "    print(tensor)\n",
    "\n",
    "torch.cat((x, y, z), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6308,  1.0930,  0.7968],\n",
      "        [-0.2834, -0.4920, -1.0990]])\n",
      "tensor([[ 0.4223, -0.4965, -1.6931, -0.0231,  2.5438],\n",
      "        [-0.7499,  0.7815,  0.7678,  0.0996,  0.8020]])\n",
      "tensor([[-0.0347],\n",
      "        [ 1.1183]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6308,  1.0930,  0.7968,  0.4223, -0.4965, -1.6931, -0.0231,  2.5438,\n",
       "         -0.0347],\n",
       "        [-0.2834, -0.4920, -1.0990, -0.7499,  0.7815,  0.7678,  0.0996,  0.8020,\n",
       "          1.1183]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2, 3)\n",
    "y = torch.randn(2, 5)\n",
    "z = torch.randn(2, 1)\n",
    "\n",
    "for tensor in [x, y, z]:\n",
    "    print(tensor)\n",
    "\n",
    "torch.cat((x, y, z), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавим дополнительную ось:\n",
    "\n",
    "[torch.unsqueeze](https://pytorch.org/docs/stable/generated/torch.unsqueeze.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0962, 0.4763, 0.1087],\n",
      "        [0.5568, 0.6149, 0.0610]])\n",
      "\n",
      "tensor([[[0.0962, 0.4763, 0.1087],\n",
      "         [0.5568, 0.6149, 0.0610]]]) torch.Size([1, 2, 3])\n",
      "\n",
      "tensor([[[0.0962, 0.4763, 0.1087]],\n",
      "\n",
      "        [[0.5568, 0.6149, 0.0610]]]) torch.Size([2, 1, 3])\n",
      "\n",
      "tensor([[[0.0962],\n",
      "         [0.4763],\n",
      "         [0.1087]],\n",
      "\n",
      "        [[0.5568],\n",
      "         [0.6149],\n",
      "         [0.0610]]]) torch.Size([2, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2, 3)\n",
    "\n",
    "print(x)\n",
    "print()\n",
    "print(x.unsqueeze(0), x.unsqueeze(0).shape)\n",
    "print()\n",
    "print(x.unsqueeze(1), x.unsqueeze(1).shape)\n",
    "print()\n",
    "print(x.unsqueeze(2), x.unsqueeze(2).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уберем лишние оси (где размер единичка):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.4401, 0.2210, 0.5720]],\n",
      "\n",
      "         [[0.4808, 0.7953, 0.3531]]]])\n",
      "\n",
      "tensor([[0.4401, 0.2210, 0.5720],\n",
      "        [0.4808, 0.7953, 0.3531]]) torch.Size([2, 3])\n",
      "\n",
      "tensor([[[0.4401, 0.2210, 0.5720]],\n",
      "\n",
      "        [[0.4808, 0.7953, 0.3531]]]) torch.Size([2, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(1, 2, 1, 3)\n",
    "\n",
    "print(x)\n",
    "print()\n",
    "print(x.squeeze(), x.squeeze().shape)\n",
    "print()\n",
    "print(x.squeeze(0), x.squeeze(0).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь поговорим про типы данных в тензорах. По умолчанию в тензорах лежат числа в torch.float32 для вещественных и torch.int64 для целочисленных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.5000, 2.2000, 3.7000, 4.9000])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([1.5, 2.2, 3.7, 4.9])\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.5000, 2.1992, 3.6992, 4.8984], dtype=torch.float16)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([1.5, 2.2, 3.7, 4.9], dtype=torch.float16)\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.5000, 2.2000, 3.7000, 4.9000], dtype=torch.float64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([1.5, 2.2, 3.7, 4.9], dtype=torch.float64)\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15, 22, 37, 49])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([15, 22, 37, 49])\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15, 22, 37, 49], dtype=torch.int32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([15, 22, 37, 49], dtype=torch.int32)\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15, 22, 37, 49], dtype=torch.int16)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([15, 22, 37, 49], dtype=torch.int16)\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Размещение тензора на GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 4050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 13 14:25:59 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 546.33                 Driver Version: 546.33       CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4050 ...  WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   40C    P3              11W /  60W |      0MiB /  6141MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15, 22, 37, 49], device='cuda:0')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([15, 22, 37, 49], device=device)\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([15, 22, 37, 49])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([15, 22, 37, 49], device='cuda:0')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([15, 22, 37, 49])\n",
    "\n",
    "print(tensor)\n",
    "\n",
    "tensor = tensor.to(device)\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15, 22, 37, 49], device='cuda:0', dtype=torch.int32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.to(torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15, 22, 37, 49])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = tensor.cpu()\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9449, 1.7462, 1.4701],\n",
       "        [0.4032, 1.7100, 1.1317]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(2, 3)\n",
    "b = torch.rand(2, 3)\n",
    "\n",
    "a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m a \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 3\u001b[0m \u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "a = a.to(device)\n",
    "\n",
    "a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9449, 1.7462, 1.4701],\n",
       "        [0.4032, 1.7100, 1.1317]], device='cuda:0')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = b.to(device)\n",
    "\n",
    "a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Операции с тензорами\n",
    "\n",
    "Большая часть операций с тензорами хорошо описана в их [документации](https://pytorch.org/docs/stable/torch.html), разберем основные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4318, 0.3145, 0.7700],\n",
       "         [0.0516, 0.0378, 0.1488]]),\n",
       " tensor([[0.2520, 0.7402, 0.3188],\n",
       "         [0.2944, 0.4800, 0.7785]]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(2, 3)\n",
    "b = torch.rand(2, 3)\n",
    "\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6838, 1.0547, 1.0888],\n",
      "        [0.3460, 0.5178, 0.9273]])\n",
      "\n",
      "tensor([[0.6838, 1.0547, 1.0888],\n",
      "        [0.3460, 0.5178, 0.9273]])\n",
      "\n",
      "tensor([[0.6838, 1.0547, 1.0888],\n",
      "        [0.3460, 0.5178, 0.9273]])\n"
     ]
    }
   ],
   "source": [
    "# поэлементные\n",
    "\n",
    "print(a + b)\n",
    "\n",
    "print()\n",
    "\n",
    "print(torch.add(a, b))\n",
    "\n",
    "print()\n",
    "\n",
    "print(a.add(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1798, -0.4256,  0.4511],\n",
      "        [-0.2427, -0.4422, -0.6297]])\n",
      "\n",
      "tensor([[ 0.1798, -0.4256,  0.4511],\n",
      "        [-0.2427, -0.4422, -0.6297]])\n",
      "\n",
      "tensor([[ 0.1798, -0.4256,  0.4511],\n",
      "        [-0.2427, -0.4422, -0.6297]])\n"
     ]
    }
   ],
   "source": [
    "print(a - b)\n",
    "\n",
    "print()\n",
    "\n",
    "print(torch.sub(a, b))\n",
    "\n",
    "print()\n",
    "\n",
    "print(a.sub(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1088, 0.2328, 0.2455],\n",
      "        [0.0152, 0.0181, 0.1158]])\n",
      "\n",
      "tensor([[0.1088, 0.2328, 0.2455],\n",
      "        [0.0152, 0.0181, 0.1158]])\n",
      "\n",
      "tensor([[0.1088, 0.2328, 0.2455],\n",
      "        [0.0152, 0.0181, 0.1158]])\n"
     ]
    }
   ],
   "source": [
    "print(a * b)\n",
    "\n",
    "print()\n",
    "\n",
    "print(torch.mul(a, b))\n",
    "\n",
    "print()\n",
    "\n",
    "print(a.mul(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.7132, 0.4249, 2.4150],\n",
      "        [0.1754, 0.0787, 0.1911]])\n",
      "\n",
      "tensor([[1.7132, 0.4249, 2.4150],\n",
      "        [0.1754, 0.0787, 0.1911]])\n",
      "\n",
      "tensor([[1.7132, 0.4249, 2.4150],\n",
      "        [0.1754, 0.0787, 0.1911]])\n"
     ]
    }
   ],
   "source": [
    "print(a / b)\n",
    "\n",
    "print()\n",
    "\n",
    "print(torch.div(a, b))\n",
    "\n",
    "print()\n",
    "\n",
    "print(a.div(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.7086, 0.2657, 0.5050],\n",
       "         [0.5635, 0.8588, 0.0481]]),\n",
       " tensor([[0.6306, 0.1477, 0.4204, 0.3263],\n",
       "         [0.0226, 0.4093, 0.8177, 0.0212],\n",
       "         [0.4859, 0.8564, 0.1474, 0.0070]]),\n",
       " tensor([[0.8495, 0.5221, 0.8909, 0.8777, 0.3118],\n",
       "         [0.8941, 0.2599, 0.0095, 0.0313, 0.8499],\n",
       "         [0.7910, 0.4438, 0.6387, 0.5380, 0.0896],\n",
       "         [0.0218, 0.3322, 0.3051, 0.9221, 0.8529],\n",
       "         [0.2670, 0.2491, 0.0524, 0.5713, 0.0724]]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(2, 3)\n",
    "b = torch.rand(3, 4)\n",
    "c = torch.rand(5, 5)\n",
    "\n",
    "a, b, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6982, 0.6459, 0.5896, 0.2404],\n",
      "        [0.3980, 0.4759, 0.9462, 0.2024]]) torch.Size([2, 4])\n",
      "\n",
      "tensor([[0.6982, 0.6459, 0.5896, 0.2404],\n",
      "        [0.3980, 0.4759, 0.9462, 0.2024]]) torch.Size([2, 4])\n",
      "\n",
      "tensor(2.7426)\n",
      "\n",
      "tensor([[2.3385, 1.6856, 2.4372, 2.4053, 1.3659],\n",
      "        [2.4451, 1.2968, 1.0096, 1.0318, 2.3393],\n",
      "        [2.2055, 1.5586, 1.8941, 1.7126, 1.0938],\n",
      "        [1.0221, 1.3940, 1.3567, 2.5145, 2.3465],\n",
      "        [1.3061, 1.2829, 1.0538, 1.7706, 1.0751]])\n"
     ]
    }
   ],
   "source": [
    "# матричные операции\n",
    "\n",
    "print(a @ b, (a @ b).shape)\n",
    "\n",
    "print()\n",
    "\n",
    "print(torch.matmul(a, b), torch.matmul(a, b).shape)\n",
    "\n",
    "print()\n",
    "\n",
    "print(c.trace())\n",
    "\n",
    "print()\n",
    "\n",
    "print(c.exp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### [Автоматическое дифференцирование](https://pytorch.org/docs/stable/notes/autograd.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7076, 0.3914, 0.9870, 0.3090, 0.6262])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(5)\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3531, 0.4118, 0.7373, 0.5783, 0.4483],\n",
       "        [0.3005, 0.9868, 0.6453, 0.5094, 0.3786],\n",
       "        [0.6972, 0.2405, 0.4653, 0.5081, 0.5772]], requires_grad=True)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.rand(3, 5, requires_grad=True)\n",
    "\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.7320e-12,  3.0774e-41, -1.7366e-12])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_z = torch.empty(3)\n",
    "\n",
    "first_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.5983, 1.6304, 1.5652], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    first_z[i] = torch.sum(w[i] * x)\n",
    "\n",
    "first_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.5983, 1.6304, 1.5652], grad_fn=<SqueezeBackward3>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.matmul(x, w.t())\n",
    "\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1148, 0.7261, 0.6116], requires_grad=True)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = torch.rand(3, requires_grad=True)\n",
    "\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(v.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3244, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.sum(z * v)\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3244199752807617"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.mean((y - 2) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1052, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad=None\n",
      "\n",
      "w.grad=None\n",
      "\n",
      "z.grad=None\n",
      "\n",
      "v.grad=None\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nuke/.local/lib/python3.10/site-packages/torch/_tensor.py:1083: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  aten/src/ATen/core/TensorBody.h:477.)\n",
      "  return self._grad\n"
     ]
    }
   ],
   "source": [
    "print(f'{x.grad=}\\n')\n",
    "print(f'{w.grad=}\\n')\n",
    "print(f'{z.grad=}\\n')\n",
    "print(f'{v.grad=}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad=None\n",
      "\n",
      "w.grad=tensor([[0.0527, 0.0291, 0.0735, 0.0230, 0.0466],\n",
      "        [0.3334, 0.1844, 0.4650, 0.1456, 0.2950],\n",
      "        [0.2808, 0.1553, 0.3917, 0.1226, 0.2485]])\n",
      "\n",
      "z.grad=None\n",
      "\n",
      "v.grad=tensor([1.0370, 1.0579, 1.0155])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{x.grad=}\\n')\n",
    "print(f'{w.grad=}\\n')\n",
    "print(f'{z.grad=}\\n')\n",
    "print(f'{v.grad=}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.5492], requires_grad=True), tensor([0.4124], requires_grad=True))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(1, requires_grad=True)\n",
    "b = torch.rand(1, requires_grad=True)\n",
    "\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1368], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = (a - b)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=None\n",
      "\n",
      "b.grad=None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{a.grad=}\\n')\n",
    "print(f'{b.grad=}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=tensor([2.])\n",
      "\n",
      "b.grad=tensor([-2.])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{a.grad=}\\n')  # 1\n",
    "print(f'{b.grad=}\\n')  # -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad.zero_()\n",
    "b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0187], grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = (a - b) ** 2\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=tensor([0.])\n",
      "\n",
      "b.grad=tensor([0.])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{a.grad=}\\n')\n",
    "print(f'{b.grad=}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=tensor([0.2736])\n",
      "\n",
      "b.grad=tensor([-0.2736])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{a.grad=}\\n')  # 2 * (a - b)\n",
    "print(f'{b.grad=}\\n')  # -2 * (a - b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2736], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 * (a - b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.3279, 0.5942, 0.6495, 0.8076, 0.6961],\n",
       "         [0.7243, 0.3865, 0.4297, 0.0069, 0.3211],\n",
       "         [0.9084, 0.0009, 0.5393, 0.4543, 0.1057]], requires_grad=True),\n",
       " tensor([[0.3158, 0.7038, 0.3901, 0.7456, 0.8604],\n",
       "         [0.4813, 0.8476, 0.9554, 0.9591, 0.9958],\n",
       "         [0.5856, 0.4295, 0.9476, 0.6895, 0.3585]], requires_grad=True))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(3, 5, requires_grad=True)\n",
    "b = torch.rand(3, 5, requires_grad=True)\n",
    "\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3189, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.mean(a * b)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=None\n",
      "\n",
      "b.grad=None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{a.grad=}\\n')\n",
    "print(f'{b.grad=}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=tensor([[0.0211, 0.0469, 0.0260, 0.0497, 0.0574],\n",
      "        [0.0321, 0.0565, 0.0637, 0.0639, 0.0664],\n",
      "        [0.0390, 0.0286, 0.0632, 0.0460, 0.0239]])\n",
      "\n",
      "b.grad=tensor([[2.1862e-02, 3.9616e-02, 4.3297e-02, 5.3840e-02, 4.6404e-02],\n",
      "        [4.8289e-02, 2.5765e-02, 2.8644e-02, 4.6089e-04, 2.1404e-02],\n",
      "        [6.0560e-02, 6.2525e-05, 3.5952e-02, 3.0284e-02, 7.0479e-03]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{a.grad=}\\n')  # b / (3 * 5)\n",
    "print(f'{b.grad=}\\n')  # a / (3 * 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.1862e-02, 3.9616e-02, 4.3297e-02, 5.3840e-02, 4.6404e-02],\n",
       "        [4.8289e-02, 2.5765e-02, 2.8644e-02, 4.6089e-04, 2.1404e-02],\n",
       "        [6.0560e-02, 6.2525e-05, 3.5952e-02, 3.0284e-02, 7.0479e-03]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a / 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0211, 0.0469, 0.0260, 0.0497, 0.0574],\n",
       "        [0.0321, 0.0565, 0.0637, 0.0639, 0.0664],\n",
       "        [0.0390, 0.0286, 0.0632, 0.0460, 0.0239]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b / 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=tensor([[0.9249, 0.5178, 0.4018, 0.5241, 0.2307],\n",
      "        [0.4165, 0.5878, 0.6177, 0.4664, 0.3297],\n",
      "        [0.2164, 0.3102, 0.1307, 0.8902, 0.9583]], requires_grad=True)\n",
      "\n",
      "a.grad=None\n",
      "\n",
      "a.grad=tensor([[1.8499, 1.0356, 0.8036, 1.0483, 0.4613],\n",
      "        [0.8329, 1.1757, 1.2353, 0.9329, 0.6594],\n",
      "        [0.4329, 0.6204, 0.2613, 1.7805, 1.9166]])\n",
      "\n",
      "a.grad=tensor([[2.8499, 2.0356, 1.8036, 2.0483, 1.4613],\n",
      "        [1.8329, 2.1757, 2.2353, 1.9329, 1.6594],\n",
      "        [1.4329, 1.6204, 1.2613, 2.7805, 2.9166]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(3, 5, requires_grad=True)\n",
    "\n",
    "print(f'{a=}\\n')\n",
    "\n",
    "loss1 = torch.sum(a ** 2) # 2a\n",
    "loss2 = torch.sum(a) # 1\n",
    "\n",
    "print(f'{a.grad=}\\n')\n",
    "\n",
    "loss1.backward()\n",
    "\n",
    "print(f'{a.grad=}\\n')\n",
    "\n",
    "loss2.backward()\n",
    "\n",
    "print(f'{a.grad=}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2*a=tensor([[1.8499, 1.0356, 0.8036, 1.0483, 0.4613],\n",
      "        [0.8329, 1.1757, 1.2353, 0.9329, 0.6594],\n",
      "        [0.4329, 0.6204, 0.2613, 1.7805, 1.9166]], grad_fn=<MulBackward0>)\n",
      "\n",
      "2*a+1=tensor([[2.8499, 2.0356, 1.8036, 2.0483, 1.4613],\n",
      "        [1.8329, 2.1757, 2.2353, 1.9329, 1.6594],\n",
      "        [1.4329, 1.6204, 1.2613, 2.7805, 2.9166]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f'{2*a=}\\n')\n",
    "print(f'{2*a+1=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.9585, 0.8137, 0.7837, 0.3117, 0.5308],\n",
       "         [0.5991, 0.7310, 0.9645, 0.7374, 0.7492],\n",
       "         [0.5522, 0.8426, 0.0137, 0.8135, 0.0080]], requires_grad=True),\n",
       " tensor([[0.0779, 0.3624, 0.0938, 0.1770, 0.2233],\n",
       "         [0.3437, 0.3984, 0.9332, 0.7502, 0.0515],\n",
       "         [0.3722, 0.6910, 0.0804, 0.5425, 0.1549]]))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(3, 5, requires_grad=True)\n",
    "b = torch.rand(3, 5, requires_grad=False)\n",
    "\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.1571, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.sum(a - b)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=None\n",
      "\n",
      "b.grad=None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{a.grad=}\\n')\n",
    "print(f'{b.grad=}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "\n",
      "b.grad=None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{a.grad=}\\n')  # all ones\n",
    "print(f'{b.grad=}\\n')  # None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4873, 0.0094, 0.3410, 0.6388, 0.3698],\n",
       "         [0.3318, 0.3989, 0.4540, 0.8396, 0.7872],\n",
       "         [0.2356, 0.5495, 0.3216, 0.0462, 0.1471]], requires_grad=True),\n",
       " tensor([[0.9219, 0.8603, 0.0196, 0.7148, 0.3156],\n",
       "         [0.4160, 0.4039, 0.9161, 0.9858, 0.4589],\n",
       "         [0.9566, 0.1626, 0.5449, 0.9074, 0.7661]], requires_grad=True))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(3, 5, requires_grad=True)\n",
    "b = torch.rand(3, 5, requires_grad=True)\n",
    "\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-3.3924)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    loss = torch.sum(a - b)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [104]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4194, 0.7270, 0.6554, 0.8120, 0.2465],\n",
       "         [0.3338, 0.5301, 0.4804, 0.7732, 0.9213],\n",
       "         [0.9066, 0.9495, 0.0488, 0.6136, 0.7182]], requires_grad=True),\n",
       " tensor([[0.2233, 0.5705, 0.0119, 0.8199, 0.7503],\n",
       "         [0.4163, 0.5326, 0.9683, 0.9022, 0.6971],\n",
       "         [0.3597, 0.9322, 0.4627, 0.8851, 0.0571]], requires_grad=True))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(3, 5, requires_grad=True)\n",
    "b = torch.rand(3, 5, requires_grad=True)\n",
    "\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5469)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    loss = torch.sum(a - b)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [107]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=tensor(15.7352)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [108]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(a \u001b[38;5;241m+\u001b[39m b)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    a = torch.rand(3, 5, requires_grad=True)\n",
    "    b = torch.rand(3, 5, requires_grad=True)\n",
    "    \n",
    "    loss = torch.sum(a + b)\n",
    "    \n",
    "    print(f'{loss=}')\n",
    "    \n",
    "    loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15.7352, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss2 = torch.sum(a + b)\n",
    "\n",
    "loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=None\n",
      "\n",
      "b.grad=None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{a.grad=}\\n')\n",
    "print(f'{b.grad=}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss2.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "\n",
      "b.grad=tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{a.grad=}\\n')\n",
    "print(f'{b.grad=}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=tensor(19.4477)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [113]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(a \u001b[38;5;241m+\u001b[39m b)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    a = torch.rand(3, 5, requires_grad=True)\n",
    "    b = torch.rand(3, 5, requires_grad=True)\n",
    "    \n",
    "    loss = torch.sum(a + b)\n",
    "    \n",
    "    print(f'{loss=}')\n",
    "    \n",
    "    loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(19.4477)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss2 = torch.sum(a + b)\n",
    "\n",
    "loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def foo():\n",
    "    a = torch.rand(3, 5, requires_grad=True)\n",
    "    b = torch.rand(3, 5, requires_grad=True)\n",
    "    \n",
    "    loss = torch.mean(a + b)\n",
    "    \n",
    "    print(f'{loss=}')\n",
    "    \n",
    "    return a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=tensor(0.9730)\n"
     ]
    }
   ],
   "source": [
    "a, b = foo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0091, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(a - b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def foo():\n",
    "    a = torch.rand(3, 5, requires_grad=True)\n",
    "    b = torch.rand(3, 5, requires_grad=True)\n",
    "    \n",
    "    loss = torch.mean(a + b)\n",
    "    \n",
    "    print(f'{loss=}')\n",
    "    \n",
    "    return a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=tensor(1.0910)\n"
     ]
    }
   ],
   "source": [
    "a, b = foo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0245)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(a - b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Полносвязные слои и функции активации в `PyTorch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Полносвязный слой\n",
    "\n",
    ">$y_j = \\sum\\limits_{i=1}^{n}x_iw_{ji} + b_j$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "layer = nn.Linear(in_features=5, out_features=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=5, out_features=3, bias=True)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1036, -0.0857,  0.1891,  0.1899,  0.0061],\n",
       "        [-0.2206, -0.1663, -0.0230, -0.0566, -0.0549],\n",
       "        [-0.1880, -0.2969, -0.1313,  0.2695,  0.4457]], requires_grad=True)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.1110, -0.0980, -0.0736], requires_grad=True)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = nn.Linear(in_features=5, out_features=3, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer.__call__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3352,  0.3743,  0.8121], grad_fn=<SqueezeBackward3>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5)\n",
    "\n",
    "print(layer(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Функции активации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "> Сигмоида $f(x) = \\dfrac{1}{1 + e^{-x}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "activation = nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.7887, -2.3923,  0.1006, -1.6202,  1.0863])\n",
      "tensor([0.1432, 0.0838, 0.5251, 0.1652, 0.7477])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5)\n",
    "\n",
    "print(x)\n",
    "\n",
    "print(activation(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "> ReLU $f(x) = \\max(0, x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "activation = nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.6977,  0.5788, -1.0462, -0.7537,  0.8037])\n",
      "tensor([0.0000, 0.5788, 0.0000, 0.0000, 0.8037])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5)\n",
    "\n",
    "print(x)\n",
    "\n",
    "print(activation(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "> Leaky ReLU $f(x) = \\max(0, x) + \\alpha \\min(0, x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "activation = nn.LeakyReLU(negative_slope=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.4079, -0.8732,  0.4297,  0.5221,  0.3086])\n",
      "tensor([ 1.4079e+00, -8.7321e-04,  4.2974e-01,  5.2209e-01,  3.0863e-01])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5)\n",
    "\n",
    "print(x)\n",
    "\n",
    "print(activation(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Градиентный спуск своими руками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_features = 2\n",
    "n_objects = 300\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "w_true = torch.randn(n_features)\n",
    "b_true = torch.randn(1)\n",
    "\n",
    "x = (torch.rand(n_objects, n_features) - 0.5) * 10 * (torch.arange(n_features) * 2 + 1)\n",
    "y = torch.matmul(x, w_true) + torch.randn(n_objects) + b_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_steps = 200\n",
    "step_size = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE на шаге 1 23.65750\n",
      "MSE на шаге 2 15.53605\n",
      "MSE на шаге 3 12.11575\n",
      "MSE на шаге 4 10.36995\n",
      "MSE на шаге 5 9.30175\n",
      "MSE на шаге 6 8.55659\n",
      "MSE на шаге 7 7.99169\n",
      "MSE на шаге 8 7.53972\n",
      "MSE на шаге 9 7.16377\n",
      "MSE на шаге 10 6.84131\n",
      "MSE на шаге 11 6.55765\n",
      "MSE на шаге 12 6.30292\n",
      "MSE на шаге 13 6.07033\n",
      "MSE на шаге 14 5.85515\n",
      "MSE на шаге 15 5.65409\n",
      "MSE на шаге 16 5.46479\n",
      "MSE на шаге 17 5.28555\n",
      "MSE на шаге 18 5.11514\n",
      "MSE на шаге 19 4.95265\n",
      "MSE на шаге 20 4.79735\n",
      "MSE на шаге 21 4.64871\n",
      "MSE на шаге 31 3.45555\n",
      "MSE на шаге 41 2.65631\n",
      "MSE на шаге 51 2.11964\n",
      "MSE на шаге 61 1.75925\n",
      "MSE на шаге 71 1.51724\n",
      "MSE на шаге 81 1.35471\n",
      "MSE на шаге 91 1.24558\n",
      "MSE на шаге 101 1.17229\n",
      "MSE на шаге 111 1.12307\n",
      "MSE на шаге 121 1.09002\n",
      "MSE на шаге 131 1.06782\n",
      "MSE на шаге 141 1.05292\n",
      "MSE на шаге 151 1.04291\n",
      "MSE на шаге 161 1.03619\n",
      "MSE на шаге 171 1.03168\n",
      "MSE на шаге 181 1.02864\n",
      "MSE на шаге 191 1.02661\n"
     ]
    }
   ],
   "source": [
    "w = torch.rand(n_features, requires_grad=True)\n",
    "b = torch.rand(1, requires_grad=True)\n",
    "\n",
    "for i in range(n_steps):\n",
    "    y_pred = torch.matmul(x, w) + b\n",
    "    \n",
    "    mse = torch.mean((y_pred - y) ** 2)\n",
    "    \n",
    "    if i < 20 or i % 10 == 0:\n",
    "        print(f'MSE на шаге {i + 1} {mse.item():.5f}')\n",
    "\n",
    "    mse.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        w -= w.grad * step_size\n",
    "        b -= b.grad * step_size\n",
    "\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE на шаге 1 67.76115\n",
      "MSE на шаге 2 42.90440\n",
      "MSE на шаге 3 34.78294\n",
      "MSE на шаге 4 31.90576\n",
      "MSE на шаге 5 30.72448\n",
      "MSE на шаге 6 30.12742\n",
      "MSE на шаге 7 29.75548\n",
      "MSE на шаге 8 29.48539\n",
      "MSE на шаге 9 29.27014\n",
      "MSE на шаге 10 29.08890\n",
      "MSE на шаге 11 28.93083\n",
      "MSE на шаге 12 28.78951\n",
      "MSE на шаге 13 28.66082\n",
      "MSE на шаге 14 28.54199\n",
      "MSE на шаге 15 28.43109\n",
      "MSE на шаге 16 28.32677\n",
      "MSE на шаге 17 28.22807\n",
      "MSE на шаге 18 28.13428\n",
      "MSE на шаге 19 28.04487\n",
      "MSE на шаге 20 27.95945\n",
      "MSE на шаге 21 27.87770\n",
      "MSE на шаге 31 27.22173\n",
      "MSE на шаге 41 26.78238\n",
      "MSE на шаге 51 26.48738\n",
      "MSE на шаге 61 26.28927\n",
      "MSE на шаге 71 26.15623\n",
      "MSE на шаге 81 26.06689\n",
      "MSE на шаге 91 26.00690\n",
      "MSE на шаге 101 25.96661\n",
      "MSE на шаге 111 25.93955\n",
      "MSE на шаге 121 25.92138\n",
      "MSE на шаге 131 25.90918\n",
      "MSE на шаге 141 25.90099\n",
      "MSE на шаге 151 25.89549\n",
      "MSE на шаге 161 25.89179\n",
      "MSE на шаге 171 25.88931\n",
      "MSE на шаге 181 25.88764\n",
      "MSE на шаге 191 25.88653\n"
     ]
    }
   ],
   "source": [
    "layer = nn.Linear(in_features=n_features, out_features=1)\n",
    "\n",
    "for i in range(n_steps):\n",
    "    y_pred = layer(x)\n",
    "\n",
    "    mse = torch.mean((y_pred - y) ** 2)\n",
    "    \n",
    "    if i < 20 or i % 10 == 0:\n",
    "        print(f'MSE на шаге {i + 1} {mse.item():.5f}')\n",
    "\n",
    "    mse.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        layer.weight -= layer.weight.grad * step_size\n",
    "        layer.bias -= layer.bias.grad * step_size\n",
    "\n",
    "#     layer.weight.grad.zero_()\n",
    "#     layer.bias.grad.zero_()\n",
    "    \n",
    "    layer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.7298],\n",
       "        [-2.7292],\n",
       "        [-2.7318],\n",
       "        [-2.7311],\n",
       "        [-2.7278],\n",
       "        [-2.7308],\n",
       "        [-2.7292],\n",
       "        [-2.7330],\n",
       "        [-2.7296],\n",
       "        [-2.7168],\n",
       "        [-2.7313],\n",
       "        [-2.7349],\n",
       "        [-2.7305],\n",
       "        [-2.7286],\n",
       "        [-2.7363],\n",
       "        [-2.7269],\n",
       "        [-2.7251],\n",
       "        [-2.7222],\n",
       "        [-2.7308],\n",
       "        [-2.7260],\n",
       "        [-2.7310],\n",
       "        [-2.7279],\n",
       "        [-2.7256],\n",
       "        [-2.7202],\n",
       "        [-2.7229],\n",
       "        [-2.7306],\n",
       "        [-2.7265],\n",
       "        [-2.7337],\n",
       "        [-2.7233],\n",
       "        [-2.7287],\n",
       "        [-2.7279],\n",
       "        [-2.7318],\n",
       "        [-2.7289],\n",
       "        [-2.7257],\n",
       "        [-2.7223],\n",
       "        [-2.7243],\n",
       "        [-2.7390],\n",
       "        [-2.7176],\n",
       "        [-2.7257],\n",
       "        [-2.7269],\n",
       "        [-2.7223],\n",
       "        [-2.7276],\n",
       "        [-2.7244],\n",
       "        [-2.7321],\n",
       "        [-2.7252],\n",
       "        [-2.7309],\n",
       "        [-2.7242],\n",
       "        [-2.7299],\n",
       "        [-2.7348],\n",
       "        [-2.7298],\n",
       "        [-2.7290],\n",
       "        [-2.7337],\n",
       "        [-2.7367],\n",
       "        [-2.7189],\n",
       "        [-2.7300],\n",
       "        [-2.7280],\n",
       "        [-2.7253],\n",
       "        [-2.7337],\n",
       "        [-2.7322],\n",
       "        [-2.7274],\n",
       "        [-2.7266],\n",
       "        [-2.7305],\n",
       "        [-2.7325],\n",
       "        [-2.7221],\n",
       "        [-2.7273],\n",
       "        [-2.7294],\n",
       "        [-2.7239],\n",
       "        [-2.7206],\n",
       "        [-2.7323],\n",
       "        [-2.7229],\n",
       "        [-2.7279],\n",
       "        [-2.7308],\n",
       "        [-2.7320],\n",
       "        [-2.7193],\n",
       "        [-2.7196],\n",
       "        [-2.7275],\n",
       "        [-2.7284],\n",
       "        [-2.7304],\n",
       "        [-2.7291],\n",
       "        [-2.7330],\n",
       "        [-2.7342],\n",
       "        [-2.7195],\n",
       "        [-2.7230],\n",
       "        [-2.7376],\n",
       "        [-2.7266],\n",
       "        [-2.7182],\n",
       "        [-2.7261],\n",
       "        [-2.7240],\n",
       "        [-2.7260],\n",
       "        [-2.7287],\n",
       "        [-2.7365],\n",
       "        [-2.7253],\n",
       "        [-2.7333],\n",
       "        [-2.7305],\n",
       "        [-2.7338],\n",
       "        [-2.7322],\n",
       "        [-2.7306],\n",
       "        [-2.7302],\n",
       "        [-2.7355],\n",
       "        [-2.7345],\n",
       "        [-2.7258],\n",
       "        [-2.7270],\n",
       "        [-2.7259],\n",
       "        [-2.7286],\n",
       "        [-2.7312],\n",
       "        [-2.7181],\n",
       "        [-2.7331],\n",
       "        [-2.7254],\n",
       "        [-2.7366],\n",
       "        [-2.7318],\n",
       "        [-2.7297],\n",
       "        [-2.7317],\n",
       "        [-2.7312],\n",
       "        [-2.7204],\n",
       "        [-2.7212],\n",
       "        [-2.7324],\n",
       "        [-2.7290],\n",
       "        [-2.7303],\n",
       "        [-2.7195],\n",
       "        [-2.7187],\n",
       "        [-2.7270],\n",
       "        [-2.7324],\n",
       "        [-2.7204],\n",
       "        [-2.7305],\n",
       "        [-2.7373],\n",
       "        [-2.7283],\n",
       "        [-2.7349],\n",
       "        [-2.7276],\n",
       "        [-2.7236],\n",
       "        [-2.7314],\n",
       "        [-2.7231],\n",
       "        [-2.7292],\n",
       "        [-2.7359],\n",
       "        [-2.7278],\n",
       "        [-2.7184],\n",
       "        [-2.7301],\n",
       "        [-2.7245],\n",
       "        [-2.7376],\n",
       "        [-2.7306],\n",
       "        [-2.7318],\n",
       "        [-2.7294],\n",
       "        [-2.7340],\n",
       "        [-2.7255],\n",
       "        [-2.7266],\n",
       "        [-2.7193],\n",
       "        [-2.7233],\n",
       "        [-2.7323],\n",
       "        [-2.7328],\n",
       "        [-2.7229],\n",
       "        [-2.7287],\n",
       "        [-2.7244],\n",
       "        [-2.7332],\n",
       "        [-2.7236],\n",
       "        [-2.7283],\n",
       "        [-2.7318],\n",
       "        [-2.7252],\n",
       "        [-2.7279],\n",
       "        [-2.7341],\n",
       "        [-2.7271],\n",
       "        [-2.7242],\n",
       "        [-2.7222],\n",
       "        [-2.7299],\n",
       "        [-2.7236],\n",
       "        [-2.7349],\n",
       "        [-2.7271],\n",
       "        [-2.7251],\n",
       "        [-2.7279],\n",
       "        [-2.7245],\n",
       "        [-2.7366],\n",
       "        [-2.7269],\n",
       "        [-2.7339],\n",
       "        [-2.7342],\n",
       "        [-2.7311],\n",
       "        [-2.7286],\n",
       "        [-2.7327],\n",
       "        [-2.7341],\n",
       "        [-2.7282],\n",
       "        [-2.7376],\n",
       "        [-2.7284],\n",
       "        [-2.7178],\n",
       "        [-2.7322],\n",
       "        [-2.7360],\n",
       "        [-2.7312],\n",
       "        [-2.7323],\n",
       "        [-2.7257],\n",
       "        [-2.7263],\n",
       "        [-2.7201],\n",
       "        [-2.7289],\n",
       "        [-2.7302],\n",
       "        [-2.7270],\n",
       "        [-2.7328],\n",
       "        [-2.7259],\n",
       "        [-2.7294],\n",
       "        [-2.7368],\n",
       "        [-2.7221],\n",
       "        [-2.7358],\n",
       "        [-2.7235],\n",
       "        [-2.7357],\n",
       "        [-2.7213],\n",
       "        [-2.7244],\n",
       "        [-2.7333],\n",
       "        [-2.7313],\n",
       "        [-2.7218],\n",
       "        [-2.7315],\n",
       "        [-2.7290],\n",
       "        [-2.7316],\n",
       "        [-2.7246],\n",
       "        [-2.7298],\n",
       "        [-2.7199],\n",
       "        [-2.7257],\n",
       "        [-2.7328],\n",
       "        [-2.7349],\n",
       "        [-2.7220],\n",
       "        [-2.7311],\n",
       "        [-2.7227],\n",
       "        [-2.7199],\n",
       "        [-2.7304],\n",
       "        [-2.7326],\n",
       "        [-2.7388],\n",
       "        [-2.7322],\n",
       "        [-2.7233],\n",
       "        [-2.7243],\n",
       "        [-2.7230],\n",
       "        [-2.7367],\n",
       "        [-2.7298],\n",
       "        [-2.7324],\n",
       "        [-2.7333],\n",
       "        [-2.7226],\n",
       "        [-2.7281],\n",
       "        [-2.7237],\n",
       "        [-2.7322],\n",
       "        [-2.7293],\n",
       "        [-2.7231],\n",
       "        [-2.7207],\n",
       "        [-2.7364],\n",
       "        [-2.7344],\n",
       "        [-2.7308],\n",
       "        [-2.7364],\n",
       "        [-2.7323],\n",
       "        [-2.7260],\n",
       "        [-2.7262],\n",
       "        [-2.7301],\n",
       "        [-2.7209],\n",
       "        [-2.7212],\n",
       "        [-2.7315],\n",
       "        [-2.7257],\n",
       "        [-2.7381],\n",
       "        [-2.7270],\n",
       "        [-2.7191],\n",
       "        [-2.7275],\n",
       "        [-2.7317],\n",
       "        [-2.7330],\n",
       "        [-2.7282],\n",
       "        [-2.7277],\n",
       "        [-2.7319],\n",
       "        [-2.7232],\n",
       "        [-2.7245],\n",
       "        [-2.7219],\n",
       "        [-2.7298],\n",
       "        [-2.7199],\n",
       "        [-2.7256],\n",
       "        [-2.7252],\n",
       "        [-2.7296],\n",
       "        [-2.7335],\n",
       "        [-2.7267],\n",
       "        [-2.7314],\n",
       "        [-2.7328],\n",
       "        [-2.7273],\n",
       "        [-2.7308],\n",
       "        [-2.7331],\n",
       "        [-2.7353],\n",
       "        [-2.7304],\n",
       "        [-2.7176],\n",
       "        [-2.7261],\n",
       "        [-2.7284],\n",
       "        [-2.7234],\n",
       "        [-2.7323],\n",
       "        [-2.7328],\n",
       "        [-2.7371],\n",
       "        [-2.7287],\n",
       "        [-2.7245],\n",
       "        [-2.7244],\n",
       "        [-2.7270],\n",
       "        [-2.7324],\n",
       "        [-2.7260],\n",
       "        [-2.7295],\n",
       "        [-2.7261],\n",
       "        [-2.7358],\n",
       "        [-2.7261],\n",
       "        [-2.7282],\n",
       "        [-2.7257],\n",
       "        [-2.7248],\n",
       "        [-2.7390],\n",
       "        [-2.7267],\n",
       "        [-2.7387],\n",
       "        [-2.7252],\n",
       "        [-2.7323],\n",
       "        [-2.7375],\n",
       "        [-2.7289],\n",
       "        [-2.7218]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300, 1])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300, 300])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(layer(x) - y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(layer(x).ravel() - y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE на шаге 1 82.40031\n",
      "MSE на шаге 2 34.83059\n",
      "MSE на шаге 3 18.67639\n",
      "MSE на шаге 4 12.57786\n",
      "MSE на шаге 5 9.87140\n",
      "MSE на шаге 6 8.42349\n",
      "MSE на шаге 7 7.51441\n",
      "MSE на шаге 8 6.87739\n",
      "MSE на шаге 9 6.39865\n",
      "MSE на шаге 10 6.02125\n",
      "MSE на шаге 11 5.71254\n",
      "MSE на шаге 12 5.45201\n",
      "MSE на шаге 13 5.22616\n",
      "MSE на шаге 14 5.02584\n",
      "MSE на шаге 15 4.84476\n",
      "MSE на шаге 16 4.67857\n",
      "MSE на шаге 17 4.52421\n",
      "MSE на шаге 18 4.37955\n",
      "MSE на шаге 19 4.24304\n",
      "MSE на шаге 20 4.11358\n",
      "MSE на шаге 21 3.99035\n",
      "MSE на шаге 31 3.01138\n",
      "MSE на шаге 41 2.35799\n",
      "MSE на шаге 51 1.91931\n",
      "MSE на шаге 61 1.62472\n",
      "MSE на шаге 71 1.42689\n",
      "MSE на шаге 81 1.29405\n",
      "MSE на шаге 91 1.20484\n",
      "MSE на шаге 101 1.14493\n",
      "MSE на шаге 111 1.10470\n",
      "MSE на шаге 121 1.07768\n",
      "MSE на шаге 131 1.05954\n",
      "MSE на шаге 141 1.04736\n",
      "MSE на шаге 151 1.03917\n",
      "MSE на шаге 161 1.03368\n",
      "MSE на шаге 171 1.02999\n",
      "MSE на шаге 181 1.02751\n",
      "MSE на шаге 191 1.02585\n"
     ]
    }
   ],
   "source": [
    "layer = nn.Linear(in_features=n_features, out_features=1)\n",
    "\n",
    "for i in range(n_steps):\n",
    "    y_pred = layer(x).ravel()\n",
    "\n",
    "    mse = torch.mean((y_pred - y) ** 2)\n",
    "    \n",
    "    if i < 20 or i % 10 == 0:\n",
    "        print(f'MSE на шаге {i + 1} {mse.item():.5f}')\n",
    "\n",
    "    mse.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        layer.weight -= layer.weight.grad * step_size\n",
    "        layer.bias -= layer.bias.grad * step_size\n",
    "\n",
    "    layer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 5\n",
    "n_objects = 300\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "w_true = torch.randn(n_features)\n",
    "\n",
    "x = (torch.rand(n_objects, n_features) - 0.5) * 10 * (torch.arange(n_features) * 2 + 1)\n",
    "y = torch.matmul(x, w_true) + torch.randn(n_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 2000\n",
    "step_size = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE на шаге 1 2052.12988\n",
      "MSE на шаге 2 606.21967\n",
      "MSE на шаге 3 207.55234\n",
      "MSE на шаге 4 82.47279\n",
      "MSE на шаге 5 40.03379\n",
      "MSE на шаге 6 24.82249\n",
      "MSE на шаге 7 19.02928\n",
      "MSE на шаге 8 16.58483\n",
      "MSE на шаге 9 15.35648\n",
      "MSE на шаге 10 14.58259\n",
      "MSE на шаге 11 13.98817\n",
      "MSE на шаге 12 13.47272\n",
      "MSE на шаге 13 12.99872\n",
      "MSE на шаге 14 12.55160\n",
      "MSE на шаге 15 12.12528\n",
      "MSE на шаге 16 11.71682\n",
      "MSE на шаге 17 11.32460\n",
      "MSE на шаге 18 10.94749\n",
      "MSE на шаге 19 10.58467\n",
      "MSE на шаге 20 10.23542\n",
      "MSE на шаге 51 3.92278\n",
      "MSE на шаге 101 1.43192\n",
      "MSE на шаге 151 1.02777\n",
      "MSE на шаге 201 0.95445\n",
      "MSE на шаге 251 0.93498\n",
      "MSE на шаге 301 0.92541\n",
      "MSE на шаге 351 0.91856\n",
      "MSE на шаге 401 0.91310\n",
      "MSE на шаге 451 0.90864\n",
      "MSE на шаге 501 0.90498\n",
      "MSE на шаге 551 0.90198\n",
      "MSE на шаге 601 0.89951\n",
      "MSE на шаге 651 0.89748\n",
      "MSE на шаге 701 0.89582\n",
      "MSE на шаге 751 0.89445\n",
      "MSE на шаге 801 0.89333\n",
      "MSE на шаге 851 0.89241\n",
      "MSE на шаге 901 0.89165\n",
      "MSE на шаге 951 0.89103\n",
      "MSE на шаге 1001 0.89052\n",
      "MSE на шаге 1051 0.89010\n",
      "MSE на шаге 1101 0.88976\n",
      "MSE на шаге 1151 0.88948\n",
      "MSE на шаге 1201 0.88925\n",
      "MSE на шаге 1251 0.88906\n",
      "MSE на шаге 1301 0.88890\n",
      "MSE на шаге 1351 0.88877\n",
      "MSE на шаге 1401 0.88867\n",
      "MSE на шаге 1451 0.88858\n",
      "MSE на шаге 1501 0.88851\n",
      "MSE на шаге 1551 0.88845\n",
      "MSE на шаге 1601 0.88840\n",
      "MSE на шаге 1651 0.88836\n",
      "MSE на шаге 1701 0.88833\n",
      "MSE на шаге 1751 0.88830\n",
      "MSE на шаге 1801 0.88828\n",
      "MSE на шаге 1851 0.88826\n",
      "MSE на шаге 1901 0.88825\n",
      "MSE на шаге 1951 0.88824\n"
     ]
    }
   ],
   "source": [
    "layer = nn.Linear(in_features=n_features, out_features=1)\n",
    "\n",
    "for i in range(n_steps):\n",
    "    y_pred = layer(x).ravel()\n",
    "\n",
    "    mse = torch.mean((y_pred - y) ** 2)\n",
    "    \n",
    "    if i < 20 or i % 50 == 0:\n",
    "        print(f'MSE на шаге {i + 1} {mse.item():.5f}')\n",
    "\n",
    "    mse.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        layer.weight -= layer.weight.grad * step_size\n",
    "        layer.bias -= layer.bias.grad * step_size\n",
    "\n",
    "    layer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 2000\n",
    "step_size = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE на шаге 1 1942.53650\n",
      "MSE на шаге 2 1676.33215\n",
      "MSE на шаге 3 1462.73621\n",
      "MSE на шаге 4 1252.29272\n",
      "MSE на шаге 5 1084.68542\n",
      "MSE на шаге 6 969.74554\n",
      "MSE на шаге 7 892.42969\n",
      "MSE на шаге 8 837.99945\n",
      "MSE на шаге 9 791.54950\n",
      "MSE на шаге 10 736.85345\n",
      "MSE на шаге 11 660.66931\n",
      "MSE на шаге 12 552.94324\n",
      "MSE на шаге 13 420.34772\n",
      "MSE на шаге 14 295.80695\n",
      "MSE на шаге 15 213.59764\n",
      "MSE на шаге 16 170.35097\n",
      "MSE на шаге 17 140.10820\n",
      "MSE на шаге 18 116.49634\n",
      "MSE на шаге 19 95.18255\n",
      "MSE на шаге 20 74.59748\n",
      "MSE на шаге 51 9.93147\n",
      "MSE на шаге 101 3.76550\n",
      "MSE на шаге 151 1.81388\n",
      "MSE на шаге 201 1.21714\n",
      "MSE на шаге 251 1.03567\n",
      "MSE на шаге 301 0.97787\n",
      "MSE на шаге 351 0.95785\n",
      "MSE на шаге 401 0.94890\n",
      "MSE на шаге 451 0.94261\n",
      "MSE на шаге 501 0.93729\n",
      "MSE на шаге 551 0.93249\n",
      "MSE на шаге 601 0.92810\n",
      "MSE на шаге 651 0.92405\n",
      "MSE на шаге 701 0.92031\n",
      "MSE на шаге 751 0.91686\n",
      "MSE на шаге 801 0.91367\n",
      "MSE на шаге 851 0.91071\n",
      "MSE на шаге 901 0.90798\n",
      "MSE на шаге 951 0.90545\n",
      "MSE на шаге 1001 0.90310\n",
      "MSE на шаге 1051 0.90093\n",
      "MSE на шаге 1101 0.89892\n",
      "MSE на шаге 1151 0.89705\n",
      "MSE на шаге 1201 0.89531\n",
      "MSE на шаге 1251 0.89371\n",
      "MSE на шаге 1301 0.89224\n",
      "MSE на шаге 1351 0.89088\n",
      "MSE на шаге 1401 0.88962\n",
      "MSE на шаге 1451 0.88845\n",
      "MSE на шаге 1501 0.88737\n",
      "MSE на шаге 1551 0.88636\n",
      "MSE на шаге 1601 0.88543\n",
      "MSE на шаге 1651 0.88456\n",
      "MSE на шаге 1701 0.88375\n",
      "MSE на шаге 1751 0.88300\n",
      "MSE на шаге 1801 0.88231\n",
      "MSE на шаге 1851 0.88166\n",
      "MSE на шаге 1901 0.88105\n",
      "MSE на шаге 1951 0.88049\n"
     ]
    }
   ],
   "source": [
    "layer1 = nn.Linear(in_features=n_features, out_features=3)\n",
    "layer2 = nn.Linear(in_features=3, out_features=1)\n",
    "activation = nn.ReLU()\n",
    "\n",
    "\n",
    "for i in range(n_steps):\n",
    "    y_pred = layer2(activation(layer1(x))).ravel()\n",
    "\n",
    "    mse = torch.mean((y_pred - y) ** 2)\n",
    "\n",
    "    if i < 20 or i % 50 == 0:\n",
    "        print(f'MSE на шаге {i + 1} {mse.item():.5f}')\n",
    "\n",
    "    mse.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        layer1.weight -= layer1.weight.grad * step_size\n",
    "        layer1.bias -= layer1.bias.grad * step_size\n",
    "        layer2.weight -= layer2.weight.grad * step_size\n",
    "        layer2.bias -= layer2.bias.grad * step_size\n",
    "\n",
    "    layer1.zero_grad()\n",
    "    layer2.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Seminar 1. Intro to DL",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
